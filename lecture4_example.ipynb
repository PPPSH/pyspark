{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f4e82c4-ba77-4953-a273-a9073ba8f58e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "\n",
    "sc = pyspark.SparkContext.getOrCreate()\n",
    "# Key / Value RDD\n",
    "\n",
    "# creating Key / Value RDD\n",
    "total_by_brand = rdd.map(lambda brand: (brand, 1))\n",
    "\n",
    "# # reduceByKey(): Merge the values for each key using an associative and commutative reduce function.\n",
    "from operator import add\n",
    "rdd = sc.parallelize([(\"a\", 1), (\"b\", 1), (\"a\", 1)])\n",
    "sorted(rdd.reduceByKey(add).collect())\n",
    "[('a', 2), ('b', 1)]\n",
    "\n",
    "\n",
    "# groupByKey(): Group the values for each key in the RDD into a single sequence. Hash-partitions the resulting RDD with numPartitions partitions.\n",
    "rdd = sc.parallelize([(\"a\", 1), (\"b\", 1), (\"a\", 1)])\n",
    "sorted(rdd.groupByKey().mapValues(len).collect())\n",
    "[('a', 2), ('b', 1)]\n",
    "sorted(rdd.groupByKey().mapValues(list).collect())\n",
    "[('a', [1, 1]), ('b', [1])]\n",
    "\n",
    "\n",
    "# sortByKey(): Sorts this RDD, which is assumed to consist of (key, value) pairs.\n",
    "tmp = [('a', 1), ('b', 2), ('1', 3), ('d', 4), ('2', 5)]\n",
    "sc.parallelize(tmp).sortByKey().first()\n",
    "('1', 3)\n",
    "\n",
    "\n",
    "# keys(), values(): Create a RDD of keys or just values\n",
    "rdd = sc.parallelize([(\"a\", 1), (\"b\", 1), (\"a\", 1)])\n",
    "rdd.keys()\n",
    "['a', 'b', 'a']\n",
    "\n",
    "# join, rightOuterJoin, leftOuterJoin, cogroup, subtractByKey\n",
    "x = sc.parallelize([(\"a\", 1), (\"b\", 4)])\n",
    "y = sc.parallelize([(\"a\", 2), (\"a\", 3)])\n",
    "sorted(x.join(y).collect())\n",
    "[('a', (1, 2)), ('a', (1, 3))]\n",
    "        \n",
    "\n",
    "# Efficiency is the key for performance!!!\n",
    "# if you only need values, use mapValues() or flatMapValues()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "d85f32ce-73a2-4cc2-bd71-ebda86d4c7fb",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'PipelinedRDD' object has no attribute 'unique'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[62], line 12\u001b[0m\n\u001b[1;32m      7\u001b[0m rdd \u001b[38;5;241m=\u001b[39m sc\u001b[38;5;241m.\u001b[39mparallelize([(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124ma\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;241m1\u001b[39m),(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;241m1\u001b[39m),(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124ma\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;241m10\u001b[39m)])\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m#rdd.reduceByKey(add).collect()\u001b[39;00m\n\u001b[0;32m---> 12\u001b[0m \u001b[43mrdd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkeys\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munique\u001b[49m()\u001b[38;5;241m.\u001b[39mcollect()\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'PipelinedRDD' object has no attribute 'unique'"
     ]
    }
   ],
   "source": [
    "import pyspark\n",
    "\n",
    "from operator import add\n",
    "\n",
    "sc = pyspark.SparkContext.getOrCreate()\n",
    "\n",
    "rdd = sc.parallelize([(\"a\",1),(\"b\",1),(\"a\",10)])\n",
    "\n",
    "\n",
    "#rdd.reduceByKey(add).collect()\n",
    "\n",
    "rdd.keys().unique().collect()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "0e91c116-d3b9-4169-abb5-ae6c5dcdca38",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('a', (1, 2)), ('a', (1, 3))]"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = sc.parallelize([(\"a\", 1), (\"b\", 4)])\n",
    "y = sc.parallelize([(\"a\", 2), (\"a\", 3)])\n",
    "sorted(x.join(y).collect())\n",
    "# [('a', (1, 2)), ('a', (1, 3))]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "x.join(y).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "008bb018-5d1b-4b60-81a6-638a81e95be8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
