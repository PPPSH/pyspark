{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "08609c91-66d3-4969-a449-e1e82f236a14",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mysql 데이터베이스 연결 성공!\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"PySpark Oracle Connection\").config(\"spark.driver.extraClassPath\", \"/home/jovyan/work/mysql-connector-java-8.4.0.jar\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "try:\n",
    "    #url = \"jdbc:oracle:thin:@mysql-container:1521/test\"\n",
    "    url = \"jdbc:mysql://mysql-container:3306/test\"\n",
    "    connection = spark._jvm.java.sql.DriverManager.getConnection(url, \"pppsh\", \"1234\")\n",
    "    print(\"mysql 데이터베이스 연결 성공!\")\n",
    "    connection.close()\n",
    "except Exception as e:\n",
    "    print(\"mysql 데이터베이스 연결 실패:\", e)\n",
    "finally:\n",
    "    spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71e28088-aaf0-497b-8786-9285d3d13bf3",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import (\n",
    "    functions as f,\n",
    "    SparkSession,\n",
    "    types as t\n",
    ")\n",
    "\n",
    "# Attribution 3.0 Unported (CC BY 3.0)\n",
    "# https://www.kaggle.com/datasets/csanhueza/the-marvel-universe-social-network\n",
    "\n",
    "spark = SparkSession.builder.appName(\"df_most_popular\").getOrCreate()\n",
    "csv_file_path = \"file:///home/jovyan/work/sample/hero-network.csv\"\n",
    "# read file\n",
    "df = spark.read\\\n",
    "            .option(\"header\", \"true\")\\\n",
    "            .option(\"inferSchema\", \"true\").csv(csv_file_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b06fb23-b755-4c10-99a2-59c555a3c6ce",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "abc62b84-73f2-4973-8bb8-752f2316c269",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------+---+------+--------------------+-------------+------------------+---------+--------------------+\n",
      "|   id|      name|age|gender|               email|date_of_birth|            salary|is_active|   registration_date|\n",
      "+-----+----------+---+------+--------------------+-------------+------------------+---------+--------------------+\n",
      "|40142|CCfUflVopl| 22|  Male|CCfUflVopl@gmail.com|   1995-05-16| 92407.39332873066|    false|2023-09-24 13:32:...|\n",
      "|80773|aRcdknCgfA| 27|  Male|aRcdknCgfA@outloo...|   1976-08-10| 36622.11028766857|    false|2024-02-19 13:32:...|\n",
      "|93016|ECDQmEhUgE| 33|Female|ECDQmEhUgE@outloo...|   1971-04-22| 82296.71382900636|    false|2023-10-16 13:32:...|\n",
      "|60745|WOfRkFtwDT| 24|  Male|WOfRkFtwDT@yahoo.com|   1963-01-28| 52258.76529687612|     true|2024-01-11 13:32:...|\n",
      "|58565|kPsJrHiawB| 44|  Male|kPsJrHiawB@yahoo.com|   1976-06-21|  85310.3197032046|     true|2024-02-20 13:32:...|\n",
      "|71447|weqweVyZlI| 61|  Male|weqweVyZlI@gmail.com|   1980-08-29| 72488.14305960177|    false|2023-11-24 13:32:...|\n",
      "|51353|HxpSGYekRT| 29|Female|HxpSGYekRT@yahoo.com|   1967-11-05| 76759.52587833587|    false|2024-02-12 13:32:...|\n",
      "|17061|AGThfXhxpk| 19|Female|AGThfXhxpk@outloo...|   1983-08-07| 32765.16535679938|     true|2024-04-20 13:32:...|\n",
      "|43992|CKqMcCBlIT| 52|Female|CKqMcCBlIT@outloo...|   2000-01-16| 39310.78507775417|     true|2024-06-07 13:32:...|\n",
      "|58781|pdfXpOELIg| 61|  Male|pdfXpOELIg@yahoo.com|   1994-03-14|56889.307403408806|    false|2023-07-04 13:32:...|\n",
      "|32406|flHjGVuFWw| 30|Female|flHjGVuFWw@yahoo.com|   1997-06-25| 52029.31398762736|     true|2024-05-10 13:32:...|\n",
      "|71598|MyttzlhVVe| 18|  Male|MyttzlhVVe@yahoo.com|   2004-08-01|  69661.3352515635|    false|2023-10-02 13:32:...|\n",
      "|96651|wxmhWUPsvb| 29|  Male|wxmhWUPsvb@yahoo.com|   1987-07-04| 92946.36674945522|     true|2024-04-08 13:32:...|\n",
      "|13673|nHimJWptxR| 45|Female|nHimJWptxR@outloo...|   1979-03-02| 78912.79974804472|     true|2023-10-12 13:32:...|\n",
      "|25839|gsrvIcTxNH| 55|  Male|gsrvIcTxNH@gmail.com|   1996-12-30| 57543.60064991018|    false|2024-06-06 13:32:...|\n",
      "|88996|oTibDhLKIj| 32|Female|oTibDhLKIj@outloo...|   1963-05-04|58628.644785743905|     true|2024-05-31 13:32:...|\n",
      "|88628|LaHaEYeACs| 49|Female|LaHaEYeACs@yahoo.com|   1987-09-04| 92057.11486186723|    false|2024-05-02 13:32:...|\n",
      "|46183|qlDtKRFrGb| 37|Female|qlDtKRFrGb@outloo...|   1978-05-24|  47905.4625939603|    false|2023-07-18 13:32:...|\n",
      "| 5241|QhDUIbMTHF| 39|  Male|QhDUIbMTHF@gmail.com|   2002-03-08| 47892.81319431378|    false|2024-06-15 13:32:...|\n",
      "|44883|TKZZQhqmbG| 41|  Male|TKZZQhqmbG@outloo...|   1989-09-17|   70543.595050097|    false|2024-01-01 13:32:...|\n",
      "+-----+----------+---+------+--------------------+-------------+------------------+---------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "import random\n",
    "import string\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# SparkSession 생성\n",
    "spark = SparkSession.builder.appName(\"Sample DataFrame Generation\").getOrCreate()\n",
    "\n",
    "# 데이터 스키마 정의\n",
    "data_schema = StructType([\n",
    "    StructField(\"id\", IntegerType(), True),\n",
    "    StructField(\"name\", StringType(), True),\n",
    "    StructField(\"age\", IntegerType(), True),\n",
    "    StructField(\"gender\", StringType(), True),\n",
    "    StructField(\"email\", StringType(), True),\n",
    "    StructField(\"date_of_birth\", DateType(), True),\n",
    "    StructField(\"salary\", DoubleType(), True),  # DoubleType()으로 변경\n",
    "    StructField(\"is_active\", BooleanType(), True),\n",
    "    StructField(\"registration_date\", TimestampType(), True)\n",
    "])\n",
    "\n",
    "# 샘플 데이터 생성\n",
    "def generate_random_data(num_rows):\n",
    "    data = []\n",
    "    for _ in range(num_rows):\n",
    "        id = random.randint(1, 100000)\n",
    "        name = ''.join(random.choices(string.ascii_letters, k=10))\n",
    "        age = random.randint(18, 65)\n",
    "        gender = random.choice([\"Male\", \"Female\"])\n",
    "        email = f\"{name}@{random.choice(['gmail.com', 'yahoo.com', 'outlook.com'])}\"\n",
    "        date_of_birth = (datetime.now() - timedelta(days=random.randint(365*18, 365*65))).date()\n",
    "        salary = random.uniform(30000, 100000) # round 제거\n",
    "        is_active = random.choice([True, False])\n",
    "        registration_date = datetime.now() - timedelta(days=random.randint(0, 365)) \n",
    "        data.append((id, name, age, gender, email, date_of_birth, salary, is_active, registration_date))\n",
    "    return data\n",
    "\n",
    "# 샘플 데이터 (10,000건) 생성 및 DataFrame 생성\n",
    "sample_data = generate_random_data(1000000)\n",
    "df = spark.createDataFrame(sample_data, schema=data_schema)\n",
    "\n",
    "# 생성된 DataFrame 출력 (선택 사항)\n",
    "df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "bd42fc46-80c1-4e93-be18-871ad69ddfcf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000000"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4bed15b-6e5b-47f3-8f0c-273f23a354dc",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "df.groupBy('registration_date').count().orderBy('count').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "bfc84895-1490-4c1f-96e0-039e030dcd90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100000/1000000 rows inserted (Batch time: 1.85 seconds)\n",
      "200000/1000000 rows inserted (Batch time: 1.69 seconds)\n",
      "300000/1000000 rows inserted (Batch time: 1.86 seconds)\n",
      "400000/1000000 rows inserted (Batch time: 1.64 seconds)\n",
      "500000/1000000 rows inserted (Batch time: 1.70 seconds)\n",
      "600000/1000000 rows inserted (Batch time: 1.71 seconds)\n",
      "700000/1000000 rows inserted (Batch time: 1.79 seconds)\n",
      "800000/1000000 rows inserted (Batch time: 1.98 seconds)\n",
      "900000/1000000 rows inserted (Batch time: 1.69 seconds)\n",
      "1000000/1000000 rows inserted (Batch time: 1.69 seconds)\n",
      "\n",
      "전체 데이터 삽입 완료 (총 시간: 20.08 seconds)\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.storagelevel import StorageLevel\n",
    "import time\n",
    "\n",
    "# MySQL 연결 정보 설정\n",
    "url = \"jdbc:mysql://mysql-container:3306/test\"\n",
    "properties = {\n",
    "    \"user\": \"pppsh\",\n",
    "    \"password\": \"1234\",\n",
    "    \"driver\": \"com.mysql.cj.jdbc.Driver\",\n",
    "    \"rewriteBatchedStatements\": \"true\",  # 배치 삽입 활성화\n",
    "    \"useServerPrepStmts\": \"false\",\n",
    "    \"cachePrepStmts\": \"true\",\n",
    "    \"prepStmtCacheSize\": \"250\",\n",
    "    \"prepStmtCacheSqlLimit\": \"2048\",\n",
    "    \"useSSL\": \"false\"\n",
    "}\n",
    "\n",
    "# 데이터 삽입 및 로깅\n",
    "batch_size = 100000  # 한 번에 삽입할 row 수\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "\n",
    "\n",
    "df = df.repartition(10, col(\"id\")).cache()\n",
    "\n",
    "# DataFrame 캐싱\n",
    "#df.cache()\n",
    "# 또는 메모리 부족 시 디스크 캐싱: df.persist(StorageLevel.DISK_ONLY)\n",
    "\n",
    "total_rows = df.count()\n",
    "inserted_rows = 0\n",
    "\n",
    "for i in range(0, total_rows, batch_size):\n",
    "    batch_start_time = time.time()\n",
    "\n",
    "    batch_df = df.limit(batch_size)\n",
    "    batch_df.write.jdbc(url=url, table=\"test.new_table\", mode=\"append\", properties=properties)\n",
    "    inserted_rows += batch_size\n",
    "\n",
    "    batch_end_time = time.time()\n",
    "    batch_elapsed_time = batch_end_time - batch_start_time\n",
    "\n",
    "    print(f\"{inserted_rows}/{total_rows} rows inserted (Batch time: {batch_elapsed_time:.2f} seconds)\")\n",
    "\n",
    "# 캐시 해제\n",
    "df.unpersist()\n",
    "\n",
    "end_time = time.time()\n",
    "elapsed_time = end_time - start_time\n",
    "print(f\"\\n전체 데이터 삽입 완료 (총 시간: {elapsed_time:.2f} seconds)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9a30623-0441-48a2-8c16-e9d2d4bb34e4",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "df.groupBy('registration_date').count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61c8f3ac-2e61-4d9c-a041-a8e8cb63e915",
   "metadata": {},
   "source": [
    "# 병렬 수행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "907b0d08-53b2-413b-bfe3-b0d60f521f85",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "cfbdfea8-4b7d-43b6-8678-265934ba43d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total write time for test.new_table: 8.831433773040771 seconds\n",
      "Total execution time: 8.90052056312561 seconds\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# 데이터프레임 설정\n",
    "df = df  # 삽입할 DataFrame\n",
    "\n",
    "# 테이블 이름 설정\n",
    "table_name = \"test.new_table\"\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# Spark 설정\n",
    "spark.conf.set(\"spark.sql.shuffle.partitions\", \"10\")\n",
    "\n",
    "# JDBC 연결 설정\n",
    "target_url = \"jdbc:mysql://mysql-container:3306/test\"\n",
    "target_properties = {\n",
    "    \"user\": \"pppsh\",\n",
    "    \"password\": \"1234\",\n",
    "    \"driver\": \"com.mysql.cj.jdbc.Driver\",\n",
    "    \"rewriteBatchedStatements\": \"true\",\n",
    "    \"useServerPrepStmts\": \"false\",\n",
    "    \"cachePrepStmts\": \"true\",\n",
    "    \"prepStmtCacheSize\": \"250\",\n",
    "    \"prepStmtCacheSqlLimit\": \"2048\",\n",
    "    \"useSSL\": \"false\"\n",
    "}\n",
    "\n",
    "# 리파티셔닝 (필요한 경우)\n",
    "df_partitioned = df.repartition(12, 'id')\n",
    "\n",
    "# 데이터 쓰기 및 시간 측정\n",
    "write_start = time.time()\n",
    "\n",
    "# 데이터 쓰기 (최종 수정)\n",
    "df_partitioned.write.mode(\"append\").jdbc(url=target_url, table=table_name, properties=target_properties)\n",
    "\n",
    "write_end = time.time()\n",
    "print(f\"Total write time for {table_name}: {write_end - write_start} seconds\")\n",
    "\n",
    "end_time = time.time()\n",
    "total_time = end_time - start_time\n",
    "print(f\"Total execution time: {total_time} seconds\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f1d9071-5a43-4d06-b83f-028d9cedf227",
   "metadata": {},
   "source": [
    "# 캐시 + 병렬 살짝 더 느려 ? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "a5ee431c-cda2-49c6-a3ae-40da5b554c07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total write time for test.new_table: 10.109902381896973 seconds\n",
      "Total execution time: 10.22282075881958 seconds\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# 데이터프레임 설정\n",
    "df = df  # 삽입할 DataFrame\n",
    "\n",
    "# 테이블 이름 설정\n",
    "table_name = \"test.new_table\"\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# Spark 설정\n",
    "spark.conf.set(\"spark.sql.shuffle.partitions\", \"10\")\n",
    "\n",
    "# JDBC 연결 설정\n",
    "target_url = \"jdbc:mysql://mysql-container:3306/test\"\n",
    "target_properties = {\n",
    "    \"user\": \"pppsh\",\n",
    "    \"password\": \"1234\",\n",
    "    \"driver\": \"com.mysql.cj.jdbc.Driver\",\n",
    "    \"rewriteBatchedStatements\": \"true\",\n",
    "    \"useServerPrepStmts\": \"false\",\n",
    "    \"cachePrepStmts\": \"true\",\n",
    "    \"prepStmtCacheSize\": \"250\",\n",
    "    \"prepStmtCacheSqlLimit\": \"2048\",\n",
    "    \"useSSL\": \"false\"\n",
    "}\n",
    "\n",
    "# 리파티셔닝 및 캐싱\n",
    "df_partitioned = df.repartition(12, 'id').cache()\n",
    "\n",
    "# 데이터 쓰기 및 시간 측정\n",
    "write_start = time.time()\n",
    "\n",
    "# 데이터 쓰기\n",
    "df_partitioned.write.mode(\"append\").jdbc(url=target_url, table=table_name, properties=target_properties)\n",
    "\n",
    "write_end = time.time()\n",
    "print(f\"Total write time for {table_name}: {write_end - write_start} seconds\")\n",
    "\n",
    "# 캐시 해제\n",
    "df_partitioned.unpersist()\n",
    "\n",
    "end_time = time.time()\n",
    "total_time = end_time - start_time\n",
    "print(f\"Total execution time: {total_time} seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b27f9631-d4ce-4101-809e-4ad55c7ee920",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
